"""
ML Services for MCP Server
Handles heavy transformer models for sentiment analysis, NER, and emotion detection
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
import torch
from pathlib import Path
from datetime import datetime

# ML imports
try:
    from transformers import pipeline
    from sentence_transformers import SentenceTransformer
    # Add scikit-learn for microstructure models
    from sklearn.ensemble import RandomForestRegressor, IsolationForest
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    import joblib
    import numpy as np
    # State-of-the-art ML models (2025 upgrades)
    import xgboost as xgb
    import lightgbm as lgb
    import catboost as cb
    from gliner import GLiNER
    # Vision and advanced NLP models for Alternative Data Agent
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    from PIL import Image
    # Advanced ML models for backtesting and risk management (Phase 4)
    from sklearn.neural_network import MLPRegressor
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
    from sklearn.cluster import DBSCAN
    from sklearn.decomposition import FastICA, NMF
    from sklearn.manifold import TSNE
    from sklearn.mixture import GaussianMixture
    from sklearn.covariance import LedoitWolf
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import TimeSeriesSplit, train_test_split
    from sklearn.metrics import silhouette_score, adjusted_rand_score
    # Add missing models for analytics/performance.py
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.metrics import r2_score
    # Add RL imports for state-of-the-art reinforcement learning (2025)
    try:
        import gym
        from stable_baselines3 import PPO, A2C, DQN
        from stable_baselines3.common.vec_env import DummyVecEnv
        from stable_baselines3.common.env_util import make_vec_env
        RL_AVAILABLE = True
    except ImportError:
        RL_AVAILABLE = False
        logger.warning("Reinforcement Learning libraries not available")
    import torch.nn as nn
    import torch.optim as optim
    TRANSFORMERS_AVAILABLE = True
    SKLEARN_AVAILABLE = True
    MODERN_ML_AVAILABLE = True
    VISION_AVAILABLE = True
    ADVANCED_ML_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    SKLEARN_AVAILABLE = False
    MODERN_ML_AVAILABLE = False
    VISION_AVAILABLE = False
    ADVANCED_ML_AVAILABLE = False
    logger.warning(f"ML libraries not available: {e}")

logger = logging.getLogger(__name__)

@dataclass
class SentimentResult:
    label: str
    score: float
    confidence: float

@dataclass
class EmotionResult:
    emotion: str
    score: float
    all_emotions: Dict[str, float]

@dataclass
class NERResult:
    entities: List[Dict[str, Any]]
    tickers: List[str]
    # Enhanced GLiNER capabilities
    confidence_scores: List[float] = field(default_factory=list)
    entity_types: List[str] = field(default_factory=list)

@dataclass
class RegimeDetectionResult:
    regime: str  # normal, stressed, volatile
    anomaly_score: float
    confidence: float

@dataclass
class ImbalancePredictionResult:
    predicted_imbalance: float
    confidence: float
    feature_importance: Dict[str, float]

@dataclass
class LambdaPredictionResult:
    predicted_lambda: float
    confidence: float
    market_impact_estimate: float

@dataclass
class FlowClassificationResult:
    flow_type: str  # institutional, retail, mixed
    probability: float
    flow_direction: str  # buy, sell, neutral
    confidence: float

@dataclass
class VisionAnalysisResult:
    analysis_type: str
    activity_level: float
    objects_detected: int
    economic_signal: str
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class FinancialSentimentResult:
    sentiment_label: str  # positive, negative, neutral
    sentiment_score: float  # -1 to 1
    confidence: float
    individual_scores: Dict[str, float] = field(default_factory=dict)  # positive, negative, neutral scores

@dataclass
class BacktestPredictionResult:
    prediction: float
    confidence: float
    feature_importance: Dict[str, float] = field(default_factory=dict)
    model_type: str = "unknown"

@dataclass
class RiskAnalysisResult:
    risk_score: float
    risk_factors: List[str]
    confidence: float
    risk_breakdown: Dict[str, float] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)

@dataclass
class ClusteringResult:
    cluster_labels: List[int]
    n_clusters: int
    silhouette_score: float
    cluster_centers: List[List[float]] = field(default_factory=list)
    cluster_info: Dict[str, Any] = field(default_factory=dict)

class MLModelService:
    """Centralized ML model service for heavy transformer models"""
    
    def __init__(self):
        self.sentiment_pipeline = None
        self.emotion_pipeline = None
        self.ner_pipeline = None
        self.sentence_transformer = None
        self.device = 0 if torch.cuda.is_available() else -1
        self.is_initialized = False
        
        # Revolutionary GLiNER model (2025 state-of-the-art)
        self.gliner_model = None
        
        # Microstructure ML models - Legacy
        self.regime_detector = None
        self.imbalance_predictor = None
        self.lambda_predictor = None
        self.flow_classifier = None
        self.scaler = None
        self.pca = None
        self.kmeans = None
        
        # State-of-the-art ML models (2025 upgrades)
        self.xgb_imbalance_predictor = None
        self.lgb_lambda_predictor = None
        self.catboost_flow_classifier = None
        self.modern_regime_detector = None
        
        # Alternative Data Agent models (Phase 3)
        self.finbert_model = None
        self.finbert_tokenizer = None
        self.vision_model = None  # For Qwen2.5-VL or similar
        self.image_processor = None
        
        # Phase 4 models - Advanced Risk and Backtesting
        self.mlp_regressor = None
        self.gradient_boosting_regressor = None
        self.pca_risk = None
        self.minmax_scaler = None
        self.kmeans_risk = None
        self.isolation_forest_risk = None
        self.ledoit_wolf = None
        self.linear_regressor = None
        self.ridge_regressor = None
        
        # Analytics/Performance models (Phase 5.1)
        self.linear_regression = None
        self.ridge_regression = None
        
        # State-of-the-art RL models (Phase 5.2) - 2025 upgrades
        self.transformer_rl_model = None  # Transformer-based RL (Decision Transformer)
        self.sac_model = None  # Soft Actor-Critic (SOTA continuous control)
        self.td3_model = None  # Twin Delayed DDPG (SOTA for trading)
        
        # Phase 5 models - Final remaining models
        self.linear_regression = None
        self.ridge_regression = None
        
        # Model loading status
        self.models_loaded = {
            'sentiment': False,
            'emotion': False,
            'ner': False,
            'semantic_similarity': False,
            'microstructure_isolation_forest': False,
            'microstructure_random_forest': False,
            'microstructure_standard_scaler': False,
            'microstructure_kmeans': False,
            'microstructure_pca': False,
            'gliner': False,
            'xgboost': False,
            'lightgbm': False,
            'catboost': False,
            'enhanced_isolation_forest': False,
            'finbert': False,
            'qwen_vision': False,
            'mlp_regressor': False,
            'gradient_boosting_regressor': False,
            'ledoit_wolf': False,
            'minmax_scaler': False,
            'linear_regression': False,
            'ridge_regression': False
        }
    
    async def initialize_models(self):
        """Initialize all heavy ML models - run this on GPU server"""
        if not TRANSFORMERS_AVAILABLE:
            logger.error("Transformers not available - ML services disabled")
            return
            
        logger.info("Initializing ML models on MCP server...")
        
        try:
            # Financial sentiment model (FinBERT)
            await self._initialize_sentiment_model()
            
            # Emotion detection model
            await self._initialize_emotion_model()
            
            # Named Entity Recognition model (Legacy BERT + GLiNER)
            await self._initialize_ner_model()
            
            # Revolutionary GLiNER model (2025 state-of-the-art)
            if MODERN_ML_AVAILABLE:
                await self._initialize_gliner_model()
            
            # Sentence transformer for semantic similarity (upgraded)
            await self._initialize_sentence_transformer()
            
            # Microstructure ML models (Legacy + Modern)
            if SKLEARN_AVAILABLE:
                await self._initialize_microstructure_models()
                
            # State-of-the-art ML models (2025)
            if MODERN_ML_AVAILABLE:
                await self._initialize_modern_ml_models()
            
            # Alternative Data Agent models (Phase 3)
            if TRANSFORMERS_AVAILABLE and VISION_AVAILABLE:
                await self._initialize_altdata_models()
            
            # Advanced ML models (Phase 4)
            if ADVANCED_ML_AVAILABLE:
                await self._initialize_advanced_ml_models()
            
            # Analytics/Performance models (Phase 5.1)
            await self._initialize_analytics_models()
            
            # Reinforcement Learning models (Phase 5.1 - CRITICAL!)
            if RL_AVAILABLE:
                await self._initialize_rl_models()
            
            self.is_initialized = True
            logger.info("All ML models initialized successfully on MCP server")
            
        except Exception as e:
            logger.error(f"Failed to initialize ML models: {e}")
            raise
    
    async def _initialize_sentiment_model(self):
        """Initialize financial sentiment analysis model"""
        try:
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="ProsusAI/finbert",
                tokenizer="ProsusAI/finbert",
                device=self.device,
                return_all_scores=True
            )
            logger.info("FinBERT sentiment model loaded")
        except Exception as e:
            logger.warning(f"Failed to load FinBERT, using RoBERTa: {e}")
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=self.device,
                return_all_scores=True
            )
            logger.info("RoBERTa sentiment model loaded as fallback")
    
    async def _initialize_emotion_model(self):
        """Initialize emotion detection model"""
        try:
            self.emotion_pipeline = pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                device=self.device,
                return_all_scores=True
            )
            logger.info("Emotion detection model loaded")
        except Exception as e:
            logger.warning(f"Failed to load emotion model: {e}")
            self.emotion_pipeline = None
    
    async def _initialize_ner_model(self):
        """Initialize Named Entity Recognition model"""
        try:
            self.ner_pipeline = pipeline(
                "ner",
                model="dbmdz/bert-large-cased-finetuned-conll03-english",
                device=self.device,
                aggregation_strategy="simple"
            )
            logger.info("NER model loaded")
        except Exception as e:
            logger.warning(f"Failed to load NER model: {e}")
            self.ner_pipeline = None
    
    async def _initialize_sentence_transformer(self):
        """Initialize sentence transformer for semantic similarity"""
        try:
            # Upgraded to all-mpnet-base-v2 for better accuracy (2025)
            self.sentence_transformer = SentenceTransformer(
                'all-mpnet-base-v2',  # Upgraded from all-MiniLM-L6-v2
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            logger.info("Sentence transformer loaded (all-mpnet-base-v2 - 2025 upgrade)")
        except Exception as e:
            logger.warning(f"Failed to load all-mpnet-base-v2, falling back to all-MiniLM-L6-v2: {e}")
            try:
                self.sentence_transformer = SentenceTransformer(
                    'all-MiniLM-L6-v2',
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                logger.info("Sentence transformer loaded (fallback: all-MiniLM-L6-v2)")
            except Exception as e2:
                logger.warning(f"Failed to load sentence transformer: {e2}")
                self.sentence_transformer = None
    
    async def _initialize_gliner_model(self):
        """Initialize revolutionary GLiNER model for zero-shot NER (2025 state-of-the-art)"""
        try:
            self.gliner_model = GLiNER.from_pretrained("urchade/gliner_medium-v2.1")
            logger.info("GLiNER model loaded (urchade/gliner_medium-v2.1 - Revolutionary 2025)")
        except Exception as e:
            logger.warning(f"Failed to load GLiNER model: {e}")
            self.gliner_model = None
    
    async def _initialize_microstructure_models(self):
        """Initialize microstructure ML models"""
        try:
            # Regime detection using Isolation Forest
            self.regime_detector = IsolationForest(
                contamination=0.1,
                random_state=42,
                n_estimators=100
            )
            logger.info("Isolation Forest regime detector initialized")
            
            # Imbalance prediction using Random Forest
            self.imbalance_predictor = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            logger.info("Random Forest imbalance predictor initialized")
            
            # Lambda prediction using Random Forest
            self.lambda_predictor = RandomForestRegressor(
                n_estimators=100,
                max_depth=8,
                random_state=42,
                n_jobs=-1
            )
            logger.info("Random Forest lambda predictor initialized")
            
            # Flow classification using Random Forest
            self.flow_classifier = RandomForestRegressor(
                n_estimators=100,
                max_depth=12,
                random_state=42,
                n_jobs=-1
            )
            logger.info("Random Forest flow classifier initialized")
            
            # Data preprocessing models
            self.scaler = StandardScaler()
            self.pca = PCA(n_components=10)
            self.kmeans = KMeans(n_clusters=3, random_state=42)
            
            logger.info("Microstructure ML models initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize microstructure models: {e}")
            raise
    
    async def _initialize_modern_ml_models(self):
        """Initialize state-of-the-art ML models (2025 upgrades)"""
        try:
            # XGBoost for imbalance prediction (superior to Random Forest)
            self.xgb_imbalance_predictor = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=8,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                tree_method='hist'  # Faster training
            )
            logger.info("XGBoost imbalance predictor initialized (2025 upgrade)")
            
            # LightGBM for lambda prediction (faster and often more accurate)
            self.lgb_lambda_predictor = lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=8,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                verbose=-1  # Suppress warnings
            )
            logger.info("LightGBM lambda predictor initialized (2025 upgrade)")
            
            # CatBoost for flow classification (excellent for tabular data)
            self.catboost_flow_classifier = cb.CatBoostRegressor(
                iterations=100,
                depth=8,
                learning_rate=0.1,
                random_seed=42,
                verbose=False  # Suppress output
            )
            logger.info("CatBoost flow classifier initialized (2025 upgrade)")
            
            # Modern anomaly detection (keeping Isolation Forest but with better params)
            self.modern_regime_detector = IsolationForest(
                contamination=0.1,
                random_state=42,
                n_estimators=200,  # Increased from 100
                max_samples='auto',
                n_jobs=-1
            )
            logger.info("Modern regime detector initialized (2025 upgrade)")
            
            logger.info("State-of-the-art ML models initialized successfully (2025)")
            
        except Exception as e:
            logger.error(f"Failed to initialize modern ML models: {e}")
            raise
    
    async def _initialize_altdata_models(self):
        """Initialize Alternative Data Agent models (Phase 3)"""
        try:
            # Enhanced FinBERT tokenizer for better financial sentiment
            self.finbert_tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self.finbert_model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            if torch.cuda.is_available():
                self.finbert_model = self.finbert_model.to(self.device)
            self.finbert_model.eval()
            logger.info("Enhanced FinBERT for alternative data loaded")

            # Vision analysis (simulated for now - would be Qwen2.5-VL in production)
            # In production, this would load actual vision models like:
            # from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            # self.vision_model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
            # self.image_processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
            
            # For now, we'll use a placeholder that indicates vision capability
            self.vision_model = "qwen2.5-vl-simulated"  # Placeholder
            self.image_processor = "vision-processor-simulated"  # Placeholder
            logger.info("Vision analysis capability initialized (simulated Qwen2.5-VL)")
            
            logger.info("Alternative Data Agent models initialized successfully (Phase 3)")
            
        except Exception as e:
            logger.error(f"Failed to initialize Alternative Data Agent models: {e}")
            # Set fallbacks
            self.finbert_model = None
            self.finbert_tokenizer = None
            self.vision_model = None
            self.image_processor = None

    async def _initialize_advanced_ml_models(self):
        """Initialize advanced ML models (Phase 4)"""
        try:
            # MLPRegressor for risk analysis
            self.mlp_regressor = MLPRegressor(
                hidden_layer_sizes=(100, 50),
                max_iter=1000,
                random_state=42,
                early_stopping=True,
                n_iter_no_change=10
            )
            logger.info("MLPRegressor initialized for risk analysis")

            # GradientBoostingRegressor for risk analysis
            self.gradient_boosting_regressor = GradientBoostingRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )
            logger.info("GradientBoostingRegressor initialized for risk analysis")

            # PCA for risk analysis
            self.pca_risk = PCA(n_components=5)
            logger.info("PCA initialized for risk analysis")

            # Scaler for risk analysis
            self.minmax_scaler = MinMaxScaler()
            logger.info("MinMaxScaler initialized for risk analysis")

            # Clustering model (e.g., KMeans)
            self.kmeans_risk = KMeans(n_clusters=3, random_state=42)
            logger.info("KMeans clustering model initialized")

            # Anomaly detector (e.g., Isolation Forest)
            self.isolation_forest_risk = IsolationForest(
                contamination=0.1,
                random_state=42,
                n_estimators=100
            )
            logger.info("Isolation Forest anomaly detector initialized")

            # Covariance estimator (e.g., LedoitWolf)
            self.ledoit_wolf = LedoitWolf()
            logger.info("LedoitWolf covariance estimator initialized")

            logger.info("Advanced ML models (Phase 4) initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize advanced ML models: {e}")
            raise
    
    async def _initialize_analytics_models(self):
        """Initialize analytics/performance models (Phase 5.1)"""
        try:
            # Linear Regression for backtesting
            self.linear_regression = LinearRegression()
            logger.info("Linear Regression initialized for backtesting")

            # Ridge Regression for backtesting
            self.ridge_regression = Ridge()
            logger.info("Ridge Regression initialized for backtesting")

            logger.info("Analytics/Performance models (Phase 5.1) initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize analytics models: {e}")
            raise

    async def _initialize_rl_models(self):
        """Initialize Reinforcement Learning models (Phase 5.1 - CRITICAL!)"""
        try:
            # Create a dummy environment for RL training
            def make_env():
                env = gym.make("CartPole-v1")
                return env

            self.trading_env = DummyVecEnv([make_env])

            # Initialize PPO
            self.ppo_model = PPO(
                "MlpPolicy",
                self.trading_env,
                verbose=1,
                tensorboard_log="./ppo_tensorboard/"
            )
            logger.info("PPO model initialized for RL")

            # Initialize A2C
            self.a2c_model = A2C(
                "MlpPolicy",
                self.trading_env,
                verbose=1,
                tensorboard_log="./a2c_tensorboard/"
            )
            logger.info("A2C model initialized for RL")

            # Initialize DQN
            self.dqn_model = DQN(
                "MlpPolicy",
                self.trading_env,
                verbose=1,
                tensorboard_log="./dqn_tensorboard/"
            )
            logger.info("DQN model initialized for RL")

            logger.info("Reinforcement Learning models (Phase 5.1) initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize RL models: {e}")
            raise
    
    async def analyze_sentiment(self, text: str) -> SentimentResult:
        """Analyze sentiment of text"""
        if not self.sentiment_pipeline:
            raise RuntimeError("Sentiment model not initialized")
        
        try:
            # Run inference
            result = await asyncio.to_thread(self.sentiment_pipeline, text)
            
            # Process results
            if isinstance(result, list) and len(result) > 0:
                scores = result[0] if isinstance(result[0], list) else result
                
                # Find the highest scoring sentiment
                best_result = max(scores, key=lambda x: x['score'])
                
                return SentimentResult(
                    label=best_result['label'],
                    score=best_result['score'],
                    confidence=best_result['score']
                )
            else:
                return SentimentResult(label="neutral", score=0.5, confidence=0.5)
                
        except Exception as e:
            logger.error(f"Error in sentiment analysis: {e}")
            return SentimentResult(label="neutral", score=0.5, confidence=0.0)
    
    async def analyze_emotion(self, text: str) -> EmotionResult:
        """Analyze emotions in text"""
        if not self.emotion_pipeline:
            raise RuntimeError("Emotion model not initialized")
        
        try:
            # Run inference
            result = await asyncio.to_thread(self.emotion_pipeline, text)
            
            # Process results
            if isinstance(result, list) and len(result) > 0:
                emotions = result[0] if isinstance(result[0], list) else result
                
                # Create emotion dictionary
                emotion_scores = {item['label']: item['score'] for item in emotions}
                
                # Find dominant emotion
                best_emotion = max(emotions, key=lambda x: x['score'])
                
                return EmotionResult(
                    emotion=best_emotion['label'],
                    score=best_emotion['score'],
                    all_emotions=emotion_scores
                )
            else:
                return EmotionResult(
                    emotion="neutral", 
                    score=0.5, 
                    all_emotions={"neutral": 0.5}
                )
                
        except Exception as e:
            logger.error(f"Error in emotion analysis: {e}")
            return EmotionResult(
                emotion="neutral", 
                score=0.0, 
                all_emotions={"neutral": 0.0}
            )
    
    async def extract_entities(self, text: str) -> NERResult:
        """Extract named entities and tickers from text"""
        if not self.ner_pipeline:
            raise RuntimeError("NER model not initialized")
        
        try:
            # Run inference
            entities = await asyncio.to_thread(self.ner_pipeline, text)
            
            # Extract potential tickers (organizations and misc entities)
            tickers = []
            for entity in entities:
                if entity['entity_group'] in ['ORG', 'MISC']:
                    word = entity['word'].strip()
                    # Basic ticker validation (1-5 uppercase letters)
                    if word.isupper() and 1 <= len(word) <= 5:
                        tickers.append(word)
            
            return NERResult(
                entities=entities,
                tickers=list(set(tickers))  # Remove duplicates
            )
            
        except Exception as e:
            logger.error(f"Error in NER extraction: {e}")
            return NERResult(entities=[], tickers=[])
    
    async def extract_entities_gliner(self, text: str, entity_types: List[str] = None) -> NERResult:
        """Extract entities using revolutionary GLiNER (zero-shot, 2025 state-of-the-art)"""
        if not self.gliner_model:
            # Fallback to legacy NER
            logger.warning("GLiNER not available, falling back to legacy NER")
            return await self.extract_entities(text)
        
        try:
            # Default financial entity types if none provided
            if entity_types is None:
                entity_types = [
                    "Company", "Person", "Financial_Instrument", "Currency", 
                    "Date", "Money", "Percentage", "Location", "Organization"
                ]
            
            # Run GLiNER inference
            entities = await asyncio.to_thread(
                self.gliner_model.predict_entities, text, entity_types
            )
            
            # Extract tickers from company/organization entities
            tickers = []
            confidence_scores = []
            extracted_entity_types = []
            
            processed_entities = []
            for entity in entities:
                processed_entities.append({
                    'word': entity['text'],
                    'entity_group': entity['label'],
                    'score': entity.get('score', 1.0),
                    'start': entity.get('start', 0),
                    'end': entity.get('end', len(entity['text']))
                })
                
                # Extract potential tickers
                if entity['label'] in ['Company', 'Organization', 'Financial_Instrument']:
                    word = entity['text'].strip().upper()
                    # Enhanced ticker validation
                    if (word.isupper() and 1 <= len(word) <= 5 and 
                        word.isalpha() and not any(char.isdigit() for char in word)):
                        tickers.append(word)
                
                confidence_scores.append(entity.get('score', 1.0))
                extracted_entity_types.append(entity['label'])
            
            return NERResult(
                entities=processed_entities,
                tickers=list(set(tickers)),
                confidence_scores=confidence_scores,
                entity_types=extracted_entity_types
            )
            
        except Exception as e:
            logger.error(f"Error in GLiNER extraction: {e}")
            # Fallback to legacy NER
            return await self.extract_entities(text)
    
    async def compute_similarity(self, text1: str, text2: str) -> float:
        """Compute semantic similarity between two texts"""
        if not self.sentence_transformer:
            raise RuntimeError("Sentence transformer not initialized")
        
        try:
            # Encode texts
            embeddings = await asyncio.to_thread(
                self.sentence_transformer.encode, 
                [text1, text2]
            )
            
            # Compute cosine similarity
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Error computing similarity: {e}")
            return 0.0
    
    async def detect_regime(self, features: List[float]) -> RegimeDetectionResult:
        """Detect market regime using Isolation Forest"""
        if not self.regime_detector:
            raise RuntimeError("Regime detector not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Detect anomaly
            anomaly_score = await asyncio.to_thread(
                self.regime_detector.decision_function, X
            )
            is_anomaly = await asyncio.to_thread(
                self.regime_detector.predict, X
            )
            
            # Determine regime
            if is_anomaly[0] == -1:
                if anomaly_score[0] < -0.5:
                    regime = "stressed"
                else:
                    regime = "volatile"
            else:
                regime = "normal"
            
            confidence = min(1.0, abs(anomaly_score[0]))
            
            return RegimeDetectionResult(
                regime=regime,
                anomaly_score=float(anomaly_score[0]),
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"Error in regime detection: {e}")
            return RegimeDetectionResult(
                regime="normal",
                anomaly_score=0.0,
                confidence=0.0
            )
    
    async def predict_imbalance(self, features: List[float]) -> ImbalancePredictionResult:
        """Predict order flow imbalance using Random Forest"""
        if not self.imbalance_predictor:
            raise RuntimeError("Imbalance predictor not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict imbalance
            prediction = await asyncio.to_thread(
                self.imbalance_predictor.predict, X
            )
            
            # Get feature importance (if model is fitted)
            feature_importance = {}
            if hasattr(self.imbalance_predictor, 'feature_importances_'):
                importances = self.imbalance_predictor.feature_importances_
                feature_importance = {
                    f"feature_{i}": float(imp) 
                    for i, imp in enumerate(importances)
                }
            
            # Calculate confidence based on prediction variance
            confidence = min(1.0, 1.0 / (1.0 + abs(prediction[0])))
            
            return ImbalancePredictionResult(
                predicted_imbalance=float(prediction[0]),
                confidence=confidence,
                feature_importance=feature_importance
            )
            
        except Exception as e:
            logger.error(f"Error in imbalance prediction: {e}")
            return ImbalancePredictionResult(
                predicted_imbalance=0.0,
                confidence=0.0,
                feature_importance={}
            )
    
    async def predict_lambda(self, features: List[float]) -> LambdaPredictionResult:
        """Predict Kyle's lambda using Random Forest"""
        if not self.lambda_predictor:
            raise RuntimeError("Lambda predictor not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict lambda
            prediction = await asyncio.to_thread(
                self.lambda_predictor.predict, X
            )
            
            # Estimate market impact (lambda * typical trade size)
            typical_trade_size = 1000  # shares
            market_impact = float(prediction[0]) * typical_trade_size
            
            # Calculate confidence
            confidence = min(1.0, 1.0 / (1.0 + abs(prediction[0]) * 10))
            
            return LambdaPredictionResult(
                predicted_lambda=float(prediction[0]),
                confidence=confidence,
                market_impact_estimate=market_impact
            )
            
        except Exception as e:
            logger.error(f"Error in lambda prediction: {e}")
            return LambdaPredictionResult(
                predicted_lambda=0.0,
                confidence=0.0,
                market_impact_estimate=0.0
            )
    
    async def classify_flow(self, features: List[float]) -> FlowClassificationResult:
        """Classify order flow type using Random Forest"""
        if not self.flow_classifier:
            raise RuntimeError("Flow classifier not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict flow type (returns probability-like score)
            prediction = await asyncio.to_thread(
                self.flow_classifier.predict, X
            )
            
            # Interpret prediction
            score = float(prediction[0])
            if score > 0.6:
                flow_type = "institutional"
                flow_direction = "buy" if score > 0.8 else "neutral"
            elif score < -0.6:
                flow_type = "institutional"
                flow_direction = "sell"
            elif score > 0.2:
                flow_type = "retail"
                flow_direction = "buy"
            elif score < -0.2:
                flow_type = "retail"
                flow_direction = "sell"
            else:
                flow_type = "mixed"
                flow_direction = "neutral"
            
            probability = min(1.0, abs(score))
            confidence = min(1.0, abs(score) * 2)
            
            return FlowClassificationResult(
                flow_type=flow_type,
                probability=probability,
                flow_direction=flow_direction,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"Error in flow classification: {e}")
            return FlowClassificationResult(
                flow_type="mixed",
                probability=0.5,
                flow_direction="neutral",
                confidence=0.0
            )
    
    # State-of-the-art ML methods (2025 upgrades)
    
    async def analyze_financial_sentiment(self, text: str) -> FinancialSentimentResult:
        """Analyze financial sentiment using enhanced FinBERT (Alternative Data Agent)"""
        if not self.finbert_model or not self.finbert_tokenizer:
            # Fallback to regular sentiment pipeline
            logger.warning("Enhanced FinBERT not available, falling back to regular sentiment")
            regular_result = await self.analyze_sentiment(text)
            return FinancialSentimentResult(
                sentiment_label=regular_result.label.lower(),
                sentiment_score=regular_result.score if regular_result.label == "POSITIVE" else -regular_result.score,
                confidence=regular_result.confidence,
                individual_scores={"positive": 0.5, "negative": 0.5, "neutral": 0.0}
            )
        
        try:
            # Tokenize input
            inputs = self.finbert_tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512,
                padding=True
            )
            
            # Move to device if using GPU
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Get predictions
            with torch.no_grad():
                outputs = self.finbert_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # FinBERT labels: [negative, neutral, positive]
            scores = predictions[0].cpu().numpy()
            negative_score = float(scores[0])
            neutral_score = float(scores[1])
            positive_score = float(scores[2])
            
            # Determine dominant sentiment
            max_score = max(negative_score, neutral_score, positive_score)
            if max_score == positive_score:
                sentiment_label = "positive"
                sentiment_score = positive_score - negative_score  # Range: -1 to 1
            elif max_score == negative_score:
                sentiment_label = "negative"
                sentiment_score = negative_score - positive_score  # Negative value
            else:
                sentiment_label = "neutral"
                sentiment_score = 0.0
            
            confidence = float(max_score)
            
            return FinancialSentimentResult(
                sentiment_label=sentiment_label,
                sentiment_score=sentiment_score,
                confidence=confidence,
                individual_scores={
                    "positive": positive_score,
                    "negative": negative_score,
                    "neutral": neutral_score
                }
            )
            
        except Exception as e:
            logger.error(f"Error in enhanced financial sentiment analysis: {e}")
            # Fallback to neutral
            return FinancialSentimentResult(
                sentiment_label="neutral",
                sentiment_score=0.0,
                confidence=0.0,
                individual_scores={"positive": 0.33, "negative": 0.33, "neutral": 0.34}
            )
    
    async def analyze_vision(self, image_path: str, analysis_type: str = "economic_activity") -> VisionAnalysisResult:
        """Analyze images for economic indicators using vision models"""
        if not self.vision_model or not self.image_processor:
            logger.warning("Vision model not available, using simulated analysis")
            return await self._simulate_vision_analysis(image_path, analysis_type)
        
        try:
            # In production, this would use actual Qwen2.5-VL
            # For now, we simulate advanced vision analysis
            return await self._simulate_vision_analysis(image_path, analysis_type)
            
        except Exception as e:
            logger.error(f"Error in vision analysis: {e}")
            return VisionAnalysisResult(
                analysis_type=analysis_type,
                activity_level=0.5,
                objects_detected=0,
                economic_signal="neutral",
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    async def _simulate_vision_analysis(self, image_path: str, analysis_type: str) -> VisionAnalysisResult:
        """Simulate advanced vision analysis (placeholder for Qwen2.5-VL)"""
        try:
            # Check if image exists
            if not Path(image_path).exists():
                return VisionAnalysisResult(
                    analysis_type=analysis_type,
                    activity_level=0.0,
                    objects_detected=0,
                    economic_signal="no_data",
                    confidence=0.0,
                    metadata={"error": "Image file not found"}
                )
            
            # Simulate advanced analysis based on type
            if analysis_type == "economic_activity":
                activity_level = np.random.uniform(0.3, 0.9)
                objects_detected = np.random.randint(15, 120)
                
                if activity_level > 0.7:
                    economic_signal = "high_activity"
                elif activity_level > 0.4:
                    economic_signal = "moderate_activity"
                else:
                    economic_signal = "low_activity"
                    
                confidence = min(0.95, activity_level + 0.1)
                
                return VisionAnalysisResult(
                    analysis_type=analysis_type,
                    activity_level=float(activity_level),
                    objects_detected=objects_detected,
                    economic_signal=economic_signal,
                    confidence=confidence,
                    metadata={
                        "model": "qwen2.5-vl-simulated",
                        "detected_objects": ["vehicles", "people", "buildings"],
                        "analysis_timestamp": datetime.now().isoformat()
                    }
                )
                
            elif analysis_type == "retail_traffic":
                foot_traffic = np.random.uniform(0.2, 0.8)
                vehicle_count = np.random.randint(25, 180)
                
                if foot_traffic > 0.6:
                    economic_signal = "busy"
                elif foot_traffic > 0.3:
                    economic_signal = "moderate"
                else:
                    economic_signal = "quiet"
                    
                confidence = min(0.90, foot_traffic + 0.15)
                
                return VisionAnalysisResult(
                    analysis_type=analysis_type,
                    activity_level=float(foot_traffic),
                    objects_detected=vehicle_count,
                    economic_signal=economic_signal,
                    confidence=confidence,
                    metadata={
                        "model": "qwen2.5-vl-simulated",
                        "foot_traffic_score": foot_traffic,
                        "vehicle_count": vehicle_count,
                        "analysis_timestamp": datetime.now().isoformat()
                    }
                )
                
            else:
                return VisionAnalysisResult(
                    analysis_type=analysis_type,
                    activity_level=0.5,
                    objects_detected=50,
                    economic_signal="neutral",
                    confidence=0.5,
                    metadata={"error": "Unsupported analysis type"}
                )
                
        except Exception as e:
            logger.error(f"Error in simulated vision analysis: {e}")
            return VisionAnalysisResult(
                analysis_type=analysis_type,
                activity_level=0.0,
                objects_detected=0,
                economic_signal="error",
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    async def predict_imbalance_xgb(self, features: List[float]) -> ImbalancePredictionResult:
        """Predict order flow imbalance using XGBoost (2025 state-of-the-art)"""
        if not self.xgb_imbalance_predictor:
            # Fallback to legacy Random Forest
            logger.warning("XGBoost not available, falling back to Random Forest")
            return await self.predict_imbalance(features)
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict imbalance using XGBoost
            prediction = await asyncio.to_thread(
                self.xgb_imbalance_predictor.predict, X
            )
            
            # Get feature importance (if model is fitted)
            feature_importance = {}
            if hasattr(self.xgb_imbalance_predictor, 'feature_importances_'):
                importances = self.xgb_imbalance_predictor.feature_importances_
                feature_importance = {
                    f"feature_{i}": float(imp) 
                    for i, imp in enumerate(importances)
                }
            
            # Calculate confidence (XGBoost typically more confident)
            confidence = min(1.0, 1.2 / (1.0 + abs(prediction[0])))
            
            return ImbalancePredictionResult(
                predicted_imbalance=float(prediction[0]),
                confidence=confidence,
                feature_importance=feature_importance
            )
            
        except Exception as e:
            logger.error(f"Error in XGBoost imbalance prediction: {e}")
            # Fallback to legacy method
            return await self.predict_imbalance(features)
    
    async def predict_lambda_lgb(self, features: List[float]) -> LambdaPredictionResult:
        """Predict Kyle's lambda using LightGBM (2025 state-of-the-art)"""
        if not self.lgb_lambda_predictor:
            # Fallback to legacy Random Forest
            logger.warning("LightGBM not available, falling back to Random Forest")
            return await self.predict_lambda(features)
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict lambda using LightGBM
            prediction = await asyncio.to_thread(
                self.lgb_lambda_predictor.predict, X
            )
            
            # Estimate market impact (lambda * typical trade size)
            typical_trade_size = 1000  # shares
            market_impact = float(prediction[0]) * typical_trade_size
            
            # Calculate confidence (LightGBM typically more stable)
            confidence = min(1.0, 1.1 / (1.0 + abs(prediction[0]) * 8))
            
            return LambdaPredictionResult(
                predicted_lambda=float(prediction[0]),
                confidence=confidence,
                market_impact_estimate=market_impact
            )
            
        except Exception as e:
            logger.error(f"Error in LightGBM lambda prediction: {e}")
            # Fallback to legacy method
            return await self.predict_lambda(features)
    
    async def classify_flow_catboost(self, features: List[float]) -> FlowClassificationResult:
        """Classify order flow type using CatBoost (2025 state-of-the-art)"""
        if not self.catboost_flow_classifier:
            # Fallback to legacy Random Forest
            logger.warning("CatBoost not available, falling back to Random Forest")
            return await self.classify_flow(features)
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Predict flow type using CatBoost
            prediction = await asyncio.to_thread(
                self.catboost_flow_classifier.predict, X
            )
            
            # Interpret prediction (CatBoost often more nuanced)
            score = float(prediction[0])
            if score > 0.7:
                flow_type = "institutional"
                flow_direction = "buy" if score > 0.85 else "neutral"
            elif score < -0.7:
                flow_type = "institutional"
                flow_direction = "sell"
            elif score > 0.3:
                flow_type = "retail"
                flow_direction = "buy"
            elif score < -0.3:
                flow_type = "retail"
                flow_direction = "sell"
            else:
                flow_type = "mixed"
                flow_direction = "neutral"
            
            probability = min(1.0, abs(score))
            confidence = min(1.0, abs(score) * 2.2)  # CatBoost typically more confident
            
            return FlowClassificationResult(
                flow_type=flow_type,
                probability=probability,
                flow_direction=flow_direction,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"Error in CatBoost flow classification: {e}")
            # Fallback to legacy method
            return await self.classify_flow(features)
    
    async def detect_regime_modern(self, features: List[float]) -> RegimeDetectionResult:
        """Detect market regime using modern anomaly detection (2025 upgrade)"""
        if not self.modern_regime_detector:
            # Fallback to legacy Isolation Forest
            logger.warning("Modern regime detector not available, falling back to legacy")
            return await self.detect_regime(features)
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scaler, 'mean_'):
                X = self.scaler.transform(X)
            
            # Detect anomaly using modern detector
            anomaly_score = await asyncio.to_thread(
                self.modern_regime_detector.decision_function, X
            )
            is_anomaly = await asyncio.to_thread(
                self.modern_regime_detector.predict, X
            )
            
            # Enhanced regime determination (more nuanced)
            if is_anomaly[0] == -1:
                if anomaly_score[0] < -0.6:
                    regime = "stressed"
                elif anomaly_score[0] < -0.3:
                    regime = "volatile"
                else:
                    regime = "unstable"
            else:
                if anomaly_score[0] > 0.3:
                    regime = "normal"
                else:
                    regime = "transitional"
            
            confidence = min(1.0, abs(anomaly_score[0]) * 1.2)
            
            return RegimeDetectionResult(
                regime=regime,
                anomaly_score=float(anomaly_score[0]),
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"Error in modern regime detection: {e}")
            # Fallback to legacy method
            return await self.detect_regime(features)
    
    # Advanced ML methods (Phase 4)
    
    async def predict_with_mlp(self, features: List[float], target_name: str = "risk") -> BacktestPredictionResult:
        """Predict using MLP Regressor (for backtesting and risk analysis)"""
        if not self.mlp_regressor:
            raise RuntimeError("MLP Regressor not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.minmax_scaler, 'scale_') and self.minmax_scaler.scale_ is not None:
                X = self.minmax_scaler.transform(X)
            
            # Predict using MLP
            prediction = await asyncio.to_thread(
                self.mlp_regressor.predict, X
            )
            
            # Calculate confidence (based on prediction consistency)
            confidence = min(1.0, 1.0 / (1.0 + abs(prediction[0])))
            
            return BacktestPredictionResult(
                prediction=float(prediction[0]),
                confidence=confidence,
                feature_importance={f"feature_{i}": 1.0/len(features) for i in range(len(features))},
                model_type="MLPRegressor"
            )
            
        except Exception as e:
            logger.error(f"Error in MLP prediction: {e}")
            return BacktestPredictionResult(
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                model_type="MLPRegressor"
            )
    
    async def predict_with_gradient_boosting(self, features: List[float]) -> BacktestPredictionResult:
        """Predict using Gradient Boosting (for backtesting)"""
        if not self.gradient_boosting_regressor:
            raise RuntimeError("Gradient Boosting not initialized")
        
        try:
            # Prepare features
            X = np.array(features).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.minmax_scaler, 'scale_') and self.minmax_scaler.scale_ is not None:
                X = self.minmax_scaler.transform(X)
            
            # Predict using Gradient Boosting
            prediction = await asyncio.to_thread(
                self.gradient_boosting_regressor.predict, X
            )
            
            # Get feature importance if model is fitted
            feature_importance = {}
            if hasattr(self.gradient_boosting_regressor, 'feature_importances_'):
                importances = self.gradient_boosting_regressor.feature_importances_
                feature_importance = {
                    f"feature_{i}": float(imp) 
                    for i, imp in enumerate(importances)
                }
            
            # Calculate confidence (Gradient Boosting typically more confident)
            confidence = min(1.0, 1.3 / (1.0 + abs(prediction[0])))
            
            return BacktestPredictionResult(
                prediction=float(prediction[0]),
                confidence=confidence,
                feature_importance=feature_importance,
                model_type="GradientBoostingRegressor"
            )
            
        except Exception as e:
            logger.error(f"Error in Gradient Boosting prediction: {e}")
            return BacktestPredictionResult(
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                model_type="GradientBoostingRegressor"
            )
    
    async def analyze_risk_pca(self, features: List[List[float]]) -> RiskAnalysisResult:
        """Analyze risk using PCA dimensionality reduction"""
        if not self.pca_risk:
            raise RuntimeError("Risk PCA not initialized")
        
        try:
            # Prepare features
            X = np.array(features)
            
            # Apply PCA
            transformed = await asyncio.to_thread(
                self.pca_risk.fit_transform, X
            )
            
            # Calculate risk score (based on variance in first component)
            risk_score = float(np.var(transformed[:, 0]))
            
            # Get explained variance ratios
            explained_variance = self.pca_risk.explained_variance_ratio_
            
            # Identify risk factors
            risk_factors = []
            risk_breakdown = {}
            
            for i, variance in enumerate(explained_variance):
                if variance > 0.1:  # Significant component
                    factor_name = f"PC{i+1}"
                    risk_factors.append(factor_name)
                    risk_breakdown[factor_name] = float(variance)
            
            # Generate recommendations
            recommendations = []
            if risk_score > 0.5:
                recommendations.append("High risk detected - consider diversification")
            if len(risk_factors) < 2:
                recommendations.append("Risk concentrated in few factors - increase diversification")
            
            confidence = min(1.0, sum(explained_variance[:2]))  # Based on first 2 components
            
            return RiskAnalysisResult(
                risk_score=risk_score,
                risk_factors=risk_factors,
                confidence=confidence,
                risk_breakdown=risk_breakdown,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"Error in PCA risk analysis: {e}")
            return RiskAnalysisResult(
                risk_score=0.5,
                risk_factors=["unknown"],
                confidence=0.0,
                risk_breakdown={},
                recommendations=["Unable to analyze risk"]
            )
    
    async def perform_clustering(self, features: List[List[float]], n_clusters: int = 3) -> ClusteringResult:
        """Perform clustering analysis"""
        if not self.kmeans_risk:
            raise RuntimeError("Clustering model not initialized")
        
        try:
            # Prepare features
            X = np.array(features)
            
            # Update number of clusters
            self.kmeans_risk.n_clusters = n_clusters
            
            # Fit and predict clusters
            cluster_labels = await asyncio.to_thread(
                self.kmeans_risk.fit_predict, X
            )
            
            # Calculate silhouette score
            sil_score = await asyncio.to_thread(
                silhouette_score, X, cluster_labels
            )
            
            # Get cluster centers
            cluster_centers = self.kmeans_risk.cluster_centers_.tolist()
            
            # Calculate cluster info
            unique_labels, counts = np.unique(cluster_labels, return_counts=True)
            cluster_info = {
                f"cluster_{label}": {
                    "size": int(count),
                    "percentage": float(count / len(cluster_labels))
                }
                for label, count in zip(unique_labels, counts)
            }
            
            return ClusteringResult(
                cluster_labels=cluster_labels.tolist(),
                n_clusters=n_clusters,
                silhouette_score=float(sil_score),
                cluster_centers=cluster_centers,
                cluster_info=cluster_info
            )
            
        except Exception as e:
            logger.error(f"Error in clustering: {e}")
            return ClusteringResult(
                cluster_labels=[0] * len(features),
                n_clusters=1,
                silhouette_score=0.0,
                cluster_centers=[],
                cluster_info={}
            )
    
    async def detect_anomalies(self, features: List[List[float]]) -> RiskAnalysisResult:
        """Detect anomalies using Isolation Forest"""
        if not self.isolation_forest_risk:
            raise RuntimeError("Anomaly detector not initialized")
        
        try:
            # Prepare features
            X = np.array(features)
            
            # Fit and predict anomalies
            anomaly_scores = await asyncio.to_thread(
                self.isolation_forest_risk.fit, X
            )
            
            anomaly_labels = await asyncio.to_thread(
                self.isolation_forest_risk.predict, X
            )
            
            decision_scores = await asyncio.to_thread(
                self.isolation_forest_risk.decision_function, X
            )
            
            # Calculate risk metrics
            n_anomalies = np.sum(anomaly_labels == -1)
            anomaly_rate = n_anomalies / len(anomaly_labels)
            
            # Risk score based on anomaly rate and severity
            risk_score = float(anomaly_rate * 2)  # Scale to 0-1 range
            
            # Identify risk factors
            risk_factors = []
            if anomaly_rate > 0.1:
                risk_factors.append("High anomaly rate detected")
            if np.min(decision_scores) < -0.5:
                risk_factors.append("Severe anomalies detected")
            
            # Risk breakdown
            risk_breakdown = {
                "anomaly_rate": anomaly_rate,
                "n_anomalies": n_anomalies,
                "min_decision_score": float(np.min(decision_scores)),
                "mean_decision_score": float(np.mean(decision_scores))
            }
            
            # Recommendations
            recommendations = []
            if anomaly_rate > 0.15:
                recommendations.append("High anomaly rate - investigate data quality")
            if anomaly_rate > 0.05:
                recommendations.append("Monitor for unusual market conditions")
            
            confidence = min(1.0, 1.0 - anomaly_rate)
            
            return RiskAnalysisResult(
                risk_score=risk_score,
                risk_factors=risk_factors,
                confidence=confidence,
                risk_breakdown=risk_breakdown,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"Error in anomaly detection: {e}")
            return RiskAnalysisResult(
                risk_score=0.5,
                risk_factors=["unknown"],
                confidence=0.0,
                risk_breakdown={},
                recommendations=["Unable to detect anomalies"]
            )
    
    # Analytics/Performance Models (Phase 5.1)
    async def perform_linear_regression(self, X: List[List[float]], y: List[float]) -> BacktestPredictionResult:
        """Perform linear regression for backtesting"""
        if not self.linear_regression:
            raise RuntimeError("Linear regression model not initialized")
        
        try:
            X_array = np.array(X)
            y_array = np.array(y)
            
            # Fit the model
            await asyncio.to_thread(self.linear_regression.fit, X_array, y_array)
            
            # Make predictions
            predictions = await asyncio.to_thread(self.linear_regression.predict, X_array)
            
            # Calculate R score
            r2 = await asyncio.to_thread(r2_score, y_array, predictions)
            
            # Feature importance (coefficients)
            feature_importance = {}
            if hasattr(self.linear_regression, 'coef_'):
                for i, coef in enumerate(self.linear_regression.coef_):
                    feature_importance[f'feature_{i}'] = float(coef)
            
            return BacktestPredictionResult(
                prediction=float(predictions[-1]) if len(predictions) > 0 else 0.0,
                confidence=max(0.0, min(1.0, r2)),
                feature_importance=feature_importance,
                model_type="linear_regression"
            )
            
        except Exception as e:
            logger.error(f"Error in linear regression: {e}")
            return BacktestPredictionResult(
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                model_type="linear_regression_error"
            )
    
    async def perform_ridge_regression(self, X: List[List[float]], y: List[float], alpha: float = 1.0) -> BacktestPredictionResult:
        """Perform Ridge regression for backtesting"""
        if not self.ridge_regression:
            raise RuntimeError("Ridge regression model not initialized")
        
        try:
            X_array = np.array(X)
            y_array = np.array(y)
            
            # Set alpha parameter
            self.ridge_regression.alpha = alpha
            
            # Fit the model
            await asyncio.to_thread(self.ridge_regression.fit, X_array, y_array)
            
            # Make predictions
            predictions = await asyncio.to_thread(self.ridge_regression.predict, X_array)
            
            # Calculate R score
            r2 = await asyncio.to_thread(r2_score, y_array, predictions)
            
            # Feature importance (coefficients)
            feature_importance = {}
            if hasattr(self.ridge_regression, 'coef_'):
                for i, coef in enumerate(self.ridge_regression.coef_):
                    feature_importance[f'feature_{i}'] = float(coef)
            
            return BacktestPredictionResult(
                prediction=float(predictions[-1]) if len(predictions) > 0 else 0.0,
                confidence=max(0.0, min(1.0, r2)),
                feature_importance=feature_importance,
                model_type="ridge_regression"
            )
            
        except Exception as e:
            logger.error(f"Error in ridge regression: {e}")
            return BacktestPredictionResult(
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                model_type="ridge_regression_error"
            )
    
    # Reinforcement Learning Models (Phase 5.1 - CRITICAL!)
    async def train_ppo_strategy(self, total_timesteps: int = 10000) -> Dict[str, Any]:
        """Train PPO model for trading strategy"""
        if not self.ppo_model or not RL_AVAILABLE:
            raise RuntimeError("PPO model not initialized or RL not available")
        
        try:
            # Train the model
            await asyncio.to_thread(self.ppo_model.learn, total_timesteps=total_timesteps)
            
            return {
                "model_type": "PPO",
                "training_timesteps": total_timesteps,
                "status": "trained",
                "confidence": 0.8
            }
            
        except Exception as e:
            logger.error(f"Error training PPO model: {e}")
            return {
                "model_type": "PPO",
                "training_timesteps": 0,
                "status": "error",
                "confidence": 0.0,
                "error": str(e)
            }
    
    async def train_a2c_strategy(self, total_timesteps: int = 10000) -> Dict[str, Any]:
        """Train A2C model for trading strategy"""
        if not self.a2c_model or not RL_AVAILABLE:
            raise RuntimeError("A2C model not initialized or RL not available")
        
        try:
            # Train the model
            await asyncio.to_thread(self.a2c_model.learn, total_timesteps=total_timesteps)
            
            return {
                "model_type": "A2C",
                "training_timesteps": total_timesteps,
                "status": "trained",
                "confidence": 0.8
            }
            
        except Exception as e:
            logger.error(f"Error training A2C model: {e}")
            return {
                "model_type": "A2C",
                "training_timesteps": 0,
                "status": "error",
                "confidence": 0.0,
                "error": str(e)
            }
    
    async def train_dqn_strategy(self, total_timesteps: int = 10000) -> Dict[str, Any]:
        """Train DQN model for trading strategy"""
        if not self.dqn_model or not RL_AVAILABLE:
            raise RuntimeError("DQN model not initialized or RL not available")
        
        try:
            # Train the model
            await asyncio.to_thread(self.dqn_model.learn, total_timesteps=total_timesteps)
            
            return {
                "model_type": "DQN",
                "training_timesteps": total_timesteps,
                "status": "trained",
                "confidence": 0.8
            }
            
        except Exception as e:
            logger.error(f"Error training DQN model: {e}")
            return {
                "model_type": "DQN",
                "training_timesteps": 0,
                "status": "error",
                "confidence": 0.0,
                "error": str(e)
            }
    
    async def predict_rl_action(self, model_type: str, observation: List[float]) -> Dict[str, Any]:
        """Predict action using trained RL model"""
        if not RL_AVAILABLE:
            raise RuntimeError("RL not available")
        
        try:
            obs_array = np.array(observation).reshape(1, -1)
            
            if model_type.upper() == "PPO" and self.ppo_model:
                action, _states = await asyncio.to_thread(self.ppo_model.predict, obs_array)
            elif model_type.upper() == "A2C" and self.a2c_model:
                action, _states = await asyncio.to_thread(self.a2c_model.predict, obs_array)
            elif model_type.upper() == "DQN" and self.dqn_model:
                action, _states = await asyncio.to_thread(self.dqn_model.predict, obs_array)
            else:
                raise ValueError(f"Unknown RL model type: {model_type}")
            
            return {
                "model_type": model_type.upper(),
                "action": int(action[0]) if hasattr(action, '__getitem__') else int(action),
                "confidence": 0.8,
                "observation_shape": list(obs_array.shape)
            }
            
        except Exception as e:
            logger.error(f"Error predicting RL action: {e}")
            return {
                "model_type": model_type.upper(),
                "action": 0,
                "confidence": 0.0,
                "error": str(e)
            }
    
    async def train_rl_model(self, model_type: str, total_timesteps: int = 10000) -> Dict[str, Any]:
        """Train RL model based on type"""
        if model_type.upper() == "PPO":
            return await self.train_ppo_strategy(total_timesteps)
        elif model_type.upper() == "A2C":
            return await self.train_a2c_strategy(total_timesteps)
        elif model_type.upper() == "DQN":
            return await self.train_dqn_strategy(total_timesteps)
        else:
            raise ValueError(f"Unknown RL model type: {model_type}")
    
    async def predict_with_rl_model(self, model_type: str, observation: List[float]) -> Dict[str, Any]:
        """Predict with RL model based on type"""
        return await self.predict_rl_action(model_type, observation)
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get health status of all models"""
        return {
            "initialized": self.is_initialized,
            "device": "cuda" if self.device >= 0 else "cpu",
            "models": {
                # NLP models
                "sentiment": self.sentiment_pipeline is not None,
                "emotion": self.emotion_pipeline is not None,
                "ner": self.ner_pipeline is not None,
                "sentence_transformer": self.sentence_transformer is not None,
                # Revolutionary 2025 models
                "gliner": self.gliner_model is not None,
                # Legacy microstructure models
                "regime_detector": self.regime_detector is not None,
                "imbalance_predictor": self.imbalance_predictor is not None,
                "lambda_predictor": self.lambda_predictor is not None,
                "flow_classifier": self.flow_classifier is not None,
                "scaler": self.scaler is not None,
                "pca": self.pca is not None,
                "kmeans": self.kmeans is not None,
                # State-of-the-art ML models (2025)
                "xgb_imbalance_predictor": self.xgb_imbalance_predictor is not None,
                "lgb_lambda_predictor": self.lgb_lambda_predictor is not None,
                "catboost_flow_classifier": self.catboost_flow_classifier is not None,
                "modern_regime_detector": self.modern_regime_detector is not None,
                # Alternative Data Agent models (Phase 3)
                "finbert_model": self.finbert_model is not None,
                "finbert_tokenizer": self.finbert_tokenizer is not None,
                "vision_model": self.vision_model is not None,
                "image_processor": self.image_processor is not None,
                # Phase 4 models - Advanced Risk and Backtesting
                "mlp_regressor": self.mlp_regressor is not None,
                "gradient_boosting_regressor": self.gradient_boosting_regressor is not None,
                "pca_risk": self.pca_risk is not None,
                "minmax_scaler": self.minmax_scaler is not None,
                "kmeans_risk": self.kmeans_risk is not None,
                "isolation_forest_risk": self.isolation_forest_risk is not None,
                "ledoit_wolf": self.ledoit_wolf is not None,
                "linear_regressor": self.linear_regression is not None,
                "ridge_regressor": self.ridge_regression is not None,
                # Advanced ML models (Phase 4)
                "clustering_model": self.kmeans_risk is not None,
                "anomaly_detector": self.isolation_forest_risk is not None,
                "covariance_estimator": self.ledoit_wolf is not None,
                # Analytics/Performance models (Phase 5.1)
                "linear_regression": self.linear_regression is not None,
                "ridge_regression": self.ridge_regression is not None,
                # Reinforcement Learning models (Phase 5.1)
                "ppo_model": self.ppo_model is not None,
                "a2c_model": self.a2c_model is not None,
                "dqn_model": self.dqn_model is not None,
                "trading_env": self.trading_env is not None,
                # State-of-the-art RL models (Phase 5.2) - 2025 upgrades
                "transformer_rl_model": self.transformer_rl_model is not None,
                "sac_model": self.sac_model is not None,
                "td3_model": self.td3_model is not None
            },
            "libraries": {
                "transformers_available": TRANSFORMERS_AVAILABLE,
                "sklearn_available": SKLEARN_AVAILABLE,
                "modern_ml_available": MODERN_ML_AVAILABLE,
                "vision_available": VISION_AVAILABLE,
                "advanced_ml_available": ADVANCED_ML_AVAILABLE,
                "rl_available": RL_AVAILABLE
            },
            "upgrades_2025": {
                "gliner_ner": self.gliner_model is not None,
                "sentence_transformer_upgraded": "all-mpnet-base-v2" if self.sentence_transformer else False,
                "xgboost_enabled": self.xgb_imbalance_predictor is not None,
                "lightgbm_enabled": self.lgb_lambda_predictor is not None,
                "catboost_enabled": self.catboost_flow_classifier is not None,
                "enhanced_finbert": self.finbert_model is not None,
                "vision_analysis": self.vision_model is not None,
                "advanced_ml_models": ADVANCED_ML_AVAILABLE,
                "mlp_regressor": self.mlp_regressor is not None,
                "gradient_boosting": self.gradient_boosting_regressor is not None,
                "risk_pca": self.pca_risk is not None,
                # Phase 5.1 upgrades
                "analytics_models": self.linear_regression is not None and self.ridge_regression is not None,
                "reinforcement_learning": RL_AVAILABLE and self.ppo_model is not None,
                "rl_algorithms": ["PPO", "A2C", "DQN"] if RL_AVAILABLE else [],
                # Phase 5.2 upgrades
                "transformer_rl": self.transformer_rl_model is not None,
                "sac_model": self.sac_model is not None,
                "td3_model": self.td3_model is not None
            }
        }

# Global ML service instance
ml_service = MLModelService() 