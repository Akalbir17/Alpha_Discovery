# GPU-Optimized Dockerfile for Alpha Discovery MCP Server
# Designed for RunPod deployment with CUDA support

FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime

# Set working directory
WORKDIR /app

# Set environment variables for CUDA and Python
ENV PYTHONPATH=/app
ENV CUDA_VISIBLE_DEVICES=0
ENV TRANSFORMERS_CACHE=/app/cache/transformers
ENV HF_HOME=/app/cache/huggingface
ENV TORCH_HOME=/app/cache/torch

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and constraints
COPY requirements.txt .
COPY constraints.txt /tmp/constraints.txt

# Install numpy first to prevent other packages from pulling incompatible versions
RUN pip install --no-cache-dir numpy==1.26.4

# Then install the rest with constraints
RUN pip install --no-cache-dir --constraint /tmp/constraints.txt -r requirements.txt

    # Install GPU-optimized PyTorch (compatible with RunPod CUDA)
    RUN pip install --no-cache-dir \
        torch==2.5.1+cu121 \
        torchvision==0.20.1+cu121 \
        torchaudio==2.5.1+cu121 \
        --index-url https://download.pytorch.org/whl/cu121

# Install additional state-of-the-art ML packages matching requirements.txt
RUN pip install --no-cache-dir \
    transformers==4.46.3 \
    sentence-transformers==2.7.0 \
    xgboost==2.1.3 \
    lightgbm==4.5.0 \
    catboost==1.2.7 \
    gliner==0.2.8 \
    stable-baselines3==2.6.0 \
    gymnasium \
    scikit-learn \
    accelerate \
    bitsandbytes \
    nltk \
    textblob

# Copy the entire project
COPY . .

# Create cache directories
RUN mkdir -p /app/cache/transformers /app/cache/huggingface /app/cache/torch

# Pre-download and cache critical models to speed up startup
RUN python -c "from transformers import AutoTokenizer, AutoModel, pipeline; from sentence_transformers import SentenceTransformer; import torch; print('Pre-downloading critical models...'); [print('✓ FinBERT cached') if not (AutoTokenizer.from_pretrained('ProsusAI/finbert'), AutoModel.from_pretrained('ProsusAI/finbert')) else None]; [print('✓ RoBERTa sentiment cached') if not pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest') else None]; [print('✓ Sentence Transformer cached') if not SentenceTransformer('sentence-transformers/all-mpnet-base-v2') else None]; print('Model pre-caching complete!')"

# Expose ports for WebSocket MCP (8001) and HTTP ML services (8002)
EXPOSE 8001 8002

# Health check to ensure the service is running
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8002/ml-health || exit 1

# Set the default command to start the MCP server
CMD ["python", "-m", "src.mcp.mcp_server"] 