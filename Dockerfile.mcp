# GPU-Optimized Dockerfile for Alpha Discovery MCP Server
# Designed for RunPod deployment with CUDA support

FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime

# Set working directory
WORKDIR /app

# Set environment variables for CUDA and Python
ENV PYTHONPATH=/app
ENV CUDA_VISIBLE_DEVICES=0
ENV TRANSFORMERS_CACHE=/app/cache/transformers
ENV HF_HOME=/app/cache/huggingface
ENV TORCH_HOME=/app/cache/torch

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and constraints, then install Python dependencies with version constraints
COPY requirements.txt .
COPY constraints.txt /tmp/constraints.txt
RUN pip install --no-cache-dir --constraint /tmp/constraints.txt -r requirements.txt

# Install GPU-optimized PyTorch (compatible with RunPod CUDA)
RUN pip install --no-cache-dir \
    torch==2.7.1+cu121 \
    torchvision==0.20.1+cu121 \
    torchaudio==2.7.1+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# Install additional state-of-the-art ML packages matching requirements.txt
RUN pip install --no-cache-dir \
    transformers==4.44.2 \
    sentence-transformers==2.7.0 \
    xgboost==2.1.3 \
    lightgbm==4.5.0 \
    catboost==1.2.7 \
    gliner==0.2.8 \
    stable-baselines3==2.6.0 \
    gymnasium \
    scikit-learn \
    accelerate \
    bitsandbytes \
    nltk \
    textblob

# Copy the entire project
COPY . .

# Create cache directories
RUN mkdir -p /app/cache/transformers /app/cache/huggingface /app/cache/torch

# Pre-download and cache critical models to speed up startup
RUN python -c "\
from transformers import AutoTokenizer, AutoModel, pipeline; \
from sentence_transformers import SentenceTransformer; \
import torch; \
print('Pre-downloading critical models...'); \
try: \
    tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert'); \
    model = AutoModel.from_pretrained('ProsusAI/finbert'); \
    print('✓ FinBERT cached'); \
except Exception as e: \
    print(f'⚠ FinBERT caching failed: {e}'); \
try: \
    sentiment_pipeline = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest'); \
    print('✓ RoBERTa sentiment cached'); \
except Exception as e: \
    print(f'⚠ RoBERTa caching failed: {e}'); \
try: \
    sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2'); \
    print('✓ Sentence Transformer cached'); \
except Exception as e: \
    print(f'⚠ Sentence Transformer caching failed: {e}'); \
print('Model pre-caching complete!');"

# Expose ports for WebSocket MCP (8001) and HTTP ML services (8002)
EXPOSE 8001 8002

# Health check to ensure the service is running
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8002/ml-health || exit 1

# Set the default command to start the MCP server
CMD ["python", "-m", "src.mcp.mcp_server"] 